reticulate::repl_python()
mc_counties = pd.read_csv("counties.csv")
import pandas as pd
mc_counties = pd.read_csv("counties.csv")
View(mc_counties)
mc_counties = pd.read_csv("counties.csv")
View(mc_counties)
return df
def clean_column_names(df):
df.columns = [re.sub(r'\W+', '_', col).strip().lower() for col in df.columns]
return df
mc = clean_column_names(mc_counties)
import re
return df
mc = clean_column_names(mc_counties)
View(mc)
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import re
# Specify the path to EdgeDriver
edge_driver_path = "C:/Program Files/EdgeDriver/msedgedriver.exe"  # Change to the location of your EdgeDriver
# Set up the Edge driver (this will be used inside the function)
service = Service(edge_driver_path)
driver = webdriver.Edge(service=service)
# Define the web scraping function
def scrape_county_data(url):
driver.get(url)
wait = WebDriverWait(driver, 10)
try:
# Extract data from the page
county_name = wait.until(EC.presence_of_element_located((By.XPATH, "//h1[@id='CountyName']"))).text
customers_tracked = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='text-right col-xs-6 col-md-2']"))).text
customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "(//div[@class='text-right col-xs-6 col-md-2'])[2]"))).text
outage_percentage = wait.until(EC.presence_of_element_located((By.XPATH, "//div[b[text()='Outage %']]/following-sibling::div"))).text
last_updated = driver.find_element(By.CSS_SELECTOR, ".datetime").text
utility_provider = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='col-xs-12 col-md-5']/a"))).text
except:
# Handle missing data gracefully
county_name = "Not Found"
customers_tracked = "Not Found"
customers_out = "Not Found"
outage_percentage = "Not Found"
last_updated = "Not Found"
utility_provider = "Not Found"
# Return the data as a dictionary
return {
"County Name": county_name,
"Customers Tracked": customers_tracked,
"Customers Out": customers_out,
"Outage Percentage": outage_percentage,
"Utility Provider": utility_provider,
"Last Updated": last_updated
}
# Read in the CSV and clean column names
mc_counties = pd.read_csv("counties.csv")
def clean_column_names(df):
df.columns = [re.sub(r'\W+', '_', col).strip().lower() for col in df.columns]
return df
mc = clean_column_names(mc_counties)
# Loop over the URLs in the 'link' column and scrape data
all_data = []
for link in mc['link']:
county_data = scrape_county_data(link)
all_data.append(county_data)
# Convert the scraped data into a DataFrame
scraped_df = pd.DataFrame(all_data)
# Print the resulting DataFrame
print(scraped_df)
# Close the browser after scraping is done
driver.quit()
# Save the DataFrame to a CSV (if needed)
scraped_df.to_csv("scraped_data.csv", index=False)
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import re
# Specify the path to EdgeDriver
edge_driver_path = "C:/Program Files/EdgeDriver/msedgedriver.exe"  # Change to the location of your EdgeDriver
# Set up the Edge driver (this will be used inside the function)
service = Service(edge_driver_path)
driver = webdriver.Edge(service=service)
# Define the web scraping function
def scrape_county_data(url):
driver.get(url)
wait = WebDriverWait(driver, 10)
try:
# Extract data from the page
county_name = wait.until(EC.presence_of_element_located((By.XPATH, "//h1[@id='CountyName']"))).text
customers_tracked = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='text-right col-xs-6 col-md-2']"))).text
customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "(//div[@class='text-right col-xs-6 col-md-2'])[2]"))).text
outage_percentage = wait.until(EC.presence_of_element_located((By.XPATH, "//div[b[text()='Outage %']]/following-sibling::div"))).text
last_updated = driver.find_element(By.CSS_SELECTOR, ".datetime").text
utility_provider = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='col-xs-12 col-md-5']/a"))).text
except:
# Handle missing data gracefully
county_name = "Not Found"
customers_tracked = "Not Found"
customers_out = "Not Found"
outage_percentage = "Not Found"
last_updated = "Not Found"
utility_provider = "Not Found"
# Return the data as a dictionary
return {
"County Name": county_name,
"Customers Tracked": customers_tracked,
"Customers Out": customers_out,
"Outage Percentage": outage_percentage,
"Utility Provider": utility_provider,
"Last Updated": last_updated
}
# Read in the CSV and clean column names
mc_counties = pd.read_csv("counties.csv")
def clean_column_names(df):
df.columns = [re.sub(r'\W+', '_', col).strip().lower() for col in df.columns]
return df
mc = clean_column_names(mc_counties)
# Loop over the URLs in the 'link' column and scrape data
all_data = []
for link in mc['link']:
county_data = scrape_county_data(link)
all_data.append(county_data)
# Convert the scraped data into a DataFrame
scraped_df = pd.DataFrame(all_data)
# Print the resulting DataFrame
print(scraped_df)
# Close the browser after scraping is done
driver.quit()
# Save the DataFrame to a CSV (if needed)
scraped_df.to_csv("scraped_data.csv", index=False)
View(all_data)
View(scraped_df)
print(driver.page_source)
reticulate::repl_python()
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import re
# Specify the path to EdgeDriver
edge_driver_path = "C:/Program Files/EdgeDriver/msedgedriver.exe"  # Change to the location of your EdgeDriver
# Set up the Edge driver (this will be used inside the function)
service = Service(edge_driver_path)
driver = webdriver.Edge(service=service)
# Define the web scraping function
def scrape_county_data(url):
driver.get(url)
wait = WebDriverWait(driver, 10)
try:
# Extract data from the page
county_name = wait.until(EC.presence_of_element_located((By.XPATH, "//h1[@id='CountyName']"))).text
total_customers_tracked = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='text-right col-xs-6 col-md-2']"))).text
customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "(//div[@class='text-right col-xs-6 col-md-2'])[2]"))).text
outage_percentage = wait.until(EC.presence_of_element_located((By.XPATH, "//div[b[text()='Outage %']]/following-sibling::div"))).text
last_updated = driver.find_element(By.CSS_SELECTOR, ".datetime").text
utility_provider = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='col-xs-12 col-md-5']/a"))).text
except:
# Handle missing data gracefully
county_name = "Not Found"
customers_tracked = "Not Found"
customers_out = "Not Found"
outage_percentage = "Not Found"
last_updated = "Not Found"
utility_provider = "Not Found"
# Return the data as a dictionary
return {
"County Name": county_name,
"Customers Tracked": customers_tracked,
"Customers Out": customers_out,
"Outage Percentage": outage_percentage,
"Utility Provider": utility_provider,
"Last Updated": last_updated
}
# Read in the CSV and clean column names
mc_counties = pd.read_csv("counties.csv")
def clean_column_names(df):
df.columns = [re.sub(r'\W+', '_', col).strip().lower() for col in df.columns]
return df
mc = clean_column_names(mc_counties)
# Loop over the URLs in the 'link' column and scrape data
all_data = []
for link in mc['link']:
county_data = scrape_county_data(link)
all_data.append(county_data)
# Convert the scraped data into a DataFrame
scraped_df = pd.DataFrame(all_data)
# Print the resulting DataFrame
print(scraped_df)
View(scraped_df)
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import re
# Specify the path to EdgeDriver
edge_driver_path = "C:/Program Files/EdgeDriver/msedgedriver.exe"  # Change to the location of your EdgeDriver
# Set up the Edge driver (this will be used inside the function)
service = Service(edge_driver_path)
driver = webdriver.Edge(service=service)
# Define the web scraping function
def scrape_county_data(url):
driver.get(url)
wait = WebDriverWait(driver, 10)
try:
# Extract data from the page
county_name = wait.until(EC.presence_of_element_located((By.XPATH, "//h1[@id='CountyName']"))).text
total_customers_tracked = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='text-right col-xs-6 col-md-2']"))).text
customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "(//div[@class='text-right col-xs-6 col-md-2'])[2]"))).text
outage_percentage = wait.until(EC.presence_of_element_located((By.XPATH, "//div[b[text()='Outage %']]/following-sibling::div"))).text
last_updated = driver.find_element(By.CSS_SELECTOR, ".datetime").text
utility_provider = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='col-xs-12 col-md-5']/a"))).text
except:
# Handle missing data gracefully
county_name = "Not Found"
customers_tracked = "Not Found"
customers_out = "Not Found"
outage_percentage = "Not Found"
last_updated = "Not Found"
utility_provider = "Not Found"
# Return the data as a dictionary
return {
"County Name": county_name,
"Customers Tracked": total_customers_tracked,
"Customers Out": customers_out,
"Outage Percentage": outage_percentage,
"Utility Provider": utility_provider,
"Last Updated": last_updated
}
# Read in the CSV and clean column names
mc_counties = pd.read_csv("counties.csv")
def clean_column_names(df):
df.columns = [re.sub(r'\W+', '_', col).strip().lower() for col in df.columns]
return df
mc = clean_column_names(mc_counties)
# Loop over the URLs in the 'link' column and scrape data
all_data = []
for link in mc['link']:
county_data = scrape_county_data(link)
all_data.append(county_data)
# Convert the scraped data into a DataFrame
scraped_df = pd.DataFrame(all_data)
# Print the resulting DataFrame
print(scraped_df)
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import re
# Specify the path to EdgeDriver
edge_driver_path = "C:/Program Files/EdgeDriver/msedgedriver.exe"  # Change to the location of your EdgeDriver
# Set up the Edge driver (this will be used inside the function)
service = Service(edge_driver_path)
driver = webdriver.Edge(service=service)
# Define the web scraping function
def scrape_county_data(url):
driver.get(url)
wait = WebDriverWait(driver, 10)
try:
# Extract data from the page
county_name = wait.until(EC.presence_of_element_located((By.XPATH, "//h1[@id='CountyName']"))).text
total_customers_tracked = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='text-right col-xs-6 col-md-2']"))).text
customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "(//div[@class='text-right col-xs-6 col-md-2'])[2]"))).text
outage_percentage = wait.until(EC.presence_of_element_located((By.XPATH, "//div[b[text()='Outage %']]/following-sibling::div"))).text
last_updated = driver.find_element(By.CSS_SELECTOR, ".datetime").text
utility_provider_1 = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='col-xs-12 col-md-5']/a"))).text
utility_1_customers_tracked = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='text-right col-xs-6 col-md-2']"))).text
utility_1_customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "(//div[@class='text-right col-xs-6 col-md-2'])[2]"))).text
except:
# Handle missing data gracefully
county_name = "Not Found"
total_customers_tracked = "Not Found"
customers_out = "Not Found"
outage_percentage = "Not Found"
last_updated = "Not Found"
utility_provider_1 = "Not Found"
utility_1_customers_tracked = "Not Found"
utility_1_customers_out = "Not FOund"
# Return the data as a dictionary
return {
"County Name": county_name,
"Customers Tracked": total_customers_tracked,
"Customers Out": customers_out,
"Outage Percentage": outage_percentage,
"Utility Provider": utility_provider,
"Last Updated": last_updated,
"Utility Provider 1": utility_provider_1,
"Customers Tracked Utility 1": utility_1_customers_tracked,
"Customes Out Utility 1": utility_1_customers_out
}
# Read in the CSV and clean column names
mc_counties = pd.read_csv("counties.csv")
def clean_column_names(df):
df.columns = [re.sub(r'\W+', '_', col).strip().lower() for col in df.columns]
return df
mc = clean_column_names(mc_counties)
# Loop over the URLs in the 'link' column and scrape data
all_data = []
for link in mc['link']:
county_data = scrape_county_data(link)
all_data.append(county_data)
# Convert the scraped data into a DataFrame
scraped_df = pd.DataFrame(all_data)
# Print the resulting DataFrame
print(scraped_df)
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import re
# Specify the path to EdgeDriver
edge_driver_path = "C:/Program Files/EdgeDriver/msedgedriver.exe"  # Change to the location of your EdgeDriver
# Set up the Edge driver (this will be used inside the function)
service = Service(edge_driver_path)
driver = webdriver.Edge(service=service)
# Define the web scraping function
def scrape_county_data(url):
driver.get(url)
wait = WebDriverWait(driver, 10)
try:
# Extract data from the page
county_name = wait.until(EC.presence_of_element_located((By.XPATH, "//h1[@id='CountyName']"))).text
total_customers_tracked = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='text-right col-xs-6 col-md-2']"))).text
customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "(//div[@class='text-right col-xs-6 col-md-2'])[2]"))).text
outage_percentage = wait.until(EC.presence_of_element_located((By.XPATH, "//div[b[text()='Outage %']]/following-sibling::div"))).text
last_updated = driver.find_element(By.CSS_SELECTOR, ".datetime").text
utility_provider_1 = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='col-xs-12 col-md-5']/a"))).text
utility_1_customers_tracked = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='text-right col-xs-6 col-md-2']"))).text
utility_1_customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "(//div[@class='text-right col-xs-6 col-md-2'])[2]"))).text
except:
# Handle missing data gracefully
county_name = "Not Found"
total_customers_tracked = "Not Found"
customers_out = "Not Found"
outage_percentage = "Not Found"
last_updated = "Not Found"
utility_provider_1 = "Not Found"
utility_1_customers_tracked = "Not Found"
utility_1_customers_out = "Not FOund"
# Return the data as a dictionary
return {
"County Name": county_name,
"Customers Tracked": total_customers_tracked,
"Customers Out": customers_out,
"Outage Percentage": outage_percentage,
"Utility Provider": utility_provider_1,
"Last Updated": last_updated,
"Utility Provider 1": utility_provider_1,
"Customers Tracked Utility 1": utility_1_customers_tracked,
"Customes Out Utility 1": utility_1_customers_out
}
# Read in the CSV and clean column names
mc_counties = pd.read_csv("counties.csv")
def clean_column_names(df):
df.columns = [re.sub(r'\W+', '_', col).strip().lower() for col in df.columns]
return df
mc = clean_column_names(mc_counties)
# Loop over the URLs in the 'link' column and scrape data
all_data = []
for link in mc['link']:
county_data = scrape_county_data(link)
all_data.append(county_data)
# Convert the scraped data into a DataFrame
scraped_df = pd.DataFrame(all_data)
View(scraped_df)
Print(driver.page_source)
print(driver.page_source)
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import re
# Specify the path to EdgeDriver
edge_driver_path = "C:/Program Files/EdgeDriver/msedgedriver.exe"  # Change to the location of your EdgeDriver
# Set up the Edge driver (this will be used inside the function)
service = Service(edge_driver_path)
driver = webdriver.Edge(service=service)
# Define the web scraping function
def scrape_county_data(url):
driver.get(url)
wait = WebDriverWait(driver, 10)
try:
# Extract data from the page
county_name = wait.until(EC.presence_of_element_located((By.XPATH, "//h1[@id='CountyName']"))).text
total_customers_tracked = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='text-right col-xs-6 col-md-2']"))).text
customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "//div[b[text()='Customers Out']]/following-sibling::div"))).text
outage_percentage = wait.until(EC.presence_of_element_located((By.XPATH, "//div[b[text()='Outage %']]/following-sibling::div"))).text
last_updated = driver.find_element(By.CSS_SELECTOR, ".datetime").text
utility_provider_1 = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='col-xs-12 col-md-5']/a"))).text
utility_1_customers_tracked = wait.until(EC.presence_of_element_located((By.XPATH, "//div[@class='text-right col-xs-6 col-md-2']"))).text
utility_1_customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "(//div[@class='text-right col-xs-6 col-md-2'])[2]"))).text
except:
# Handle missing data gracefully
county_name = "Not Found"
total_customers_tracked = "Not Found"
customers_out = "Not Found"
outage_percentage = "Not Found"
last_updated = "Not Found"
utility_provider_1 = "Not Found"
utility_1_customers_tracked = "Not Found"
utility_1_customers_out = "Not FOund"
# Return the data as a dictionary
return {
"County Name": county_name,
"Customers Tracked": total_customers_tracked,
"Customers Out": customers_out,
"Outage Percentage": outage_percentage,
"Utility Provider": utility_provider_1,
"Last Updated": last_updated,
"Utility Provider 1": utility_provider_1,
"Customers Tracked Utility 1": utility_1_customers_tracked,
"Customes Out Utility 1": utility_1_customers_out
}
# Read in the CSV and clean column names
mc_counties = pd.read_csv("counties.csv")
def clean_column_names(df):
df.columns = [re.sub(r'\W+', '_', col).strip().lower() for col in df.columns]
return df
mc = clean_column_names(mc_counties)
# Loop over the URLs in the 'link' column and scrape data
all_data = []
for link in mc['link']:
county_data = scrape_county_data(link)
all_data.append(county_data)
# Convert the scraped data into a DataFrame
scraped_df = pd.DataFrame(all_data)
View(scraped_df)
customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "//div[b[text()='Customers Out']]/following-sibling::div"))).text
reticulate::repl_python()
customers_out = wait.until(EC.presence_of_element_located((By.XPATH, "//div[b[text()='Customers Out']]/following-sibling::div"))).text
reticulate::repl_python()
from selenium import webdriver
reticulate::repl_python()
from selenium import webdriver
reticulate::repl_python()
from selenium import webdriver
exit
quit
reticulate::repl_python()
from selenium import webdriver
print(driver.page_source)
